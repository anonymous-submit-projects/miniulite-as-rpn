{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12db7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from util import *\n",
    "import pandas as pd \n",
    "from datetime import timedelta\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "from util import count_trainable_parameters, measure_inference_speed\n",
    "\n",
    "#pip install XlsxWriter\n",
    "#jupyter nbconvert --to script Trainer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe37cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, mode='max', delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            self.sign = 1\n",
    "        else:  # 'max'\n",
    "            self.sign = -1\n",
    "\n",
    "    def step(self, score):\n",
    "        score = self.sign * score  # transform max into min if necessary\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassJaccardIndex,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    "    MulticlassF1Score,\n",
    ")\n",
    "# We are not using DiceScore for multiclass here\n",
    "# because they don't have ignore_index support yet.\n",
    "#from torchmetrics.segmentation import DiceScore, MeanIoU\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, num_classes=2, print_stats=False, criterion=None, device='cuda'):\n",
    "\n",
    "    model.eval()\n",
    "    #dice_metric      = DiceScore(num_classes=num_classes, average=\"macro\", input_format='index', aggregation_level='global').to(device)\n",
    "    #miou_metric      = MeanIoU(num_classes=num_classes, input_format='index').to(device)\n",
    "    miou_metric      = MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
    "    prec_metric      = MulticlassPrecision(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
    "    recall_metric    = MulticlassRecall(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
    "    f1_metric        = MulticlassF1Score(num_classes=num_classes, average=\"macro\", ignore_index=255).to(device)\n",
    "\n",
    "    val_loss    = 0.0\n",
    "    mIoU        = 0.0\n",
    "    precision   = 0.0\n",
    "    recall      = 0.0\n",
    "    f1          = 0.0\n",
    "    q           = 0.0\n",
    "\n",
    "    dataset_size = len(data_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images = images.to(device)\n",
    "            masks  = masks.to(device)\n",
    "            if num_classes > 2:\n",
    "                masks = masks.squeeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Loss \n",
    "            if criterion is not None:\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # sigmoid for binary, softmax for multi-class\n",
    "            if num_classes == 2:\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                preds = (preds > 0.5).long()\n",
    "            else:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            masks = masks.long()\n",
    "\n",
    "            # TorchMetrics\n",
    "            batch_mIoU      = miou_metric(preds, masks).item()\n",
    "            batch_precision = prec_metric(preds, masks).item()\n",
    "            batch_recall    = recall_metric(preds, masks).item()\n",
    "            batch_f1        = f1_metric(preds, masks).item()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            mIoU      += batch_mIoU      * batch_size\n",
    "            precision += batch_precision * batch_size\n",
    "            recall    += batch_recall    * batch_size\n",
    "            f1        += batch_f1        * batch_size\n",
    "            # Q = Dice * mIoU\n",
    "            q         += (batch_f1 * batch_mIoU) * batch_size\n",
    "\n",
    "    # Averages\n",
    "    avg_loss        = val_loss    / dataset_size if criterion else 0.0\n",
    "    avg_f1          = f1          / dataset_size\n",
    "    avg_mIoU        = mIoU        / dataset_size\n",
    "    avg_precision   = precision   / dataset_size\n",
    "    avg_recall      = recall      / dataset_size\n",
    "    avg_q           = q           / dataset_size\n",
    "\n",
    "    if print_stats:\n",
    "        print(\n",
    "            f\"Loss: {avg_loss:.4f} \"\n",
    "            f\"F1: {avg_f1:.4f} mIoU: {avg_mIoU:.4f} \"\n",
    "            f\"Prec: {avg_precision:.4f} \"\n",
    "            f\"Recall: {avg_recall:.4f} Q: {avg_q:.4f}\"\n",
    "        )\n",
    "\n",
    "    return avg_loss, avg_f1, avg_mIoU, avg_precision, avg_recall, avg_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Losses(Enum):\n",
    "    CrossEntropyLoss    = 0\n",
    "    BCEWithLogitsLoss   = 1\n",
    "    BCEDiceLoss         = 2\n",
    "\n",
    "#BCE + Dice Loss (for binary segmentation)\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # ECB (with logits)\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "\n",
    "        # Sigmoid to convert logits → probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "\n",
    "        # Flatten for Dice calculation\n",
    "        probs = probs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)\n",
    "        dice_loss = 1 - dice\n",
    "\n",
    "        # Thoughtful combination\n",
    "        loss = self.bce_weight * bce_loss + (1 - self.bce_weight) * dice_loss\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e570e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    model         = None\n",
    "    criterion     = None\n",
    "    optimizer     = None\n",
    "    scheduler     = None\n",
    "    learning_rate = None\n",
    "\n",
    "    def __init__(self, \n",
    "                 model_filename=None, \n",
    "                 model_dir=None, \n",
    "                 info={}, \n",
    "                 save_xlsx=False, \n",
    "                 load_best=True, \n",
    "                 device=None, \n",
    "                 rewrite_model=False, \n",
    "                 num_classes = 2,\n",
    "                 loss_function=Losses.BCEDiceLoss):\n",
    "\n",
    "        if save_xlsx:\n",
    "            if model_filename is None:\n",
    "                raise Exception(\"model_filename is mandatory when with save_xlsx == True\")\n",
    "        \n",
    "        self.save_xlsx     = save_xlsx\n",
    "        self.load_best     = load_best\n",
    "        self.num_classes   = num_classes\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # saves the model name and directory\n",
    "        self.model_filename = model_filename\n",
    "        if model_dir is None:\n",
    "            model_dir = model_filename\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        # if at least the model name is passed\n",
    "        if self.model_filename is not None:\n",
    "            self.model_file_dir = self.model_dir + \"/\" + self.model_filename\n",
    "            self.hist_name = self.model_file_dir.replace('.pth', '.xlsx')\n",
    "            self.best_path           = self.model_file_dir.replace('.pth', '-best.pth')\n",
    "            self.last_path           = self.model_file_dir.replace('.pth', '-last.pth')\n",
    "        else:\n",
    "            self.model_file_dir = None\n",
    "\n",
    "        if rewrite_model and self.model_file_dir is not None:\n",
    "            if os.path.exists(self.hist_name):\n",
    "                os.remove(self.hist_name)\n",
    "            if os.path.exists(self.model_file_dir):\n",
    "                os.remove(self.model_file_dir)\n",
    "            if os.path.exists(self.best_path):\n",
    "                os.remove(self.best_path)\n",
    "            if os.path.exists(self.last_path):\n",
    "                os.remove(self.last_path)\n",
    "\n",
    "        # extra information to be saved in xlsx\n",
    "        self.info = info\n",
    "        # index of the sample image that will be used\n",
    "        # to save output during training\n",
    "        self.sample_img_fixed_index = 0\n",
    "        # Makes some initializations\n",
    "        self.create_criterion()\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        print(\"Device:\",self.device)\n",
    "    \n",
    "\n",
    "    def create_criterion(self):\n",
    "        # If the loss is present in the Losses enum\n",
    "        if isinstance(self.loss_function, Losses):    \n",
    "            self.info['loss_function'] = self.loss_function\n",
    "\n",
    "            if self.loss_function == Losses.CrossEntropyLoss:\n",
    "                self.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "            elif self.loss_function == Losses.BCEWithLogitsLoss:\n",
    "                self.criterion = nn.BCEWithLogitsLoss()\n",
    "            elif self.loss_function == Losses.BCEDiceLoss:\n",
    "                self.criterion = BCEDiceLoss(bce_weight=0.5)\n",
    "\n",
    "        # otherwise you cand pass a custom function\n",
    "        elif isinstance(self.loss_function, FunctionType):\n",
    "            self.info['loss_function'] = self.loss_function.__name__\n",
    "            self.criterion = self.loss_function\n",
    "\n",
    "        elif isinstance(self.loss_function, object):\n",
    "            self.info['loss_function'] = self.loss_function.__class__.__name__\n",
    "            self.criterion             = self.loss_function\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_scheduler(self, patience=10, factor=0.5, mode='max'):\n",
    "        self.info['scheduler'] = \"ReduceLROnPlateau(optimizer, mode='max', patience=10, factor=0.5, verbose=True)\"\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, \n",
    "                                      mode=mode, \n",
    "                                      patience=patience, \n",
    "                                      factor=factor)\n",
    "        \n",
    "        \n",
    "\n",
    "    def create_optimizer(self):\n",
    "        self.info['optimizer'] = f\"optim.Adam(self.model.parameters(), lr={self.learning_rate})\"\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def train_loop(self, images, masks, epoch):\n",
    "        outputs     = self.model(images)\n",
    "\n",
    "        if self.num_classes == 2:\n",
    "            outputs   = outputs.squeeze(1)\n",
    "            masks_s   = masks.squeeze(1).float()\n",
    "        else:\n",
    "            outputs   = self.model(images)\n",
    "            masks_s   = masks.long().squeeze(1)\n",
    "\n",
    "        loss    = self.criterion(outputs, masks_s)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_loss = loss.item() * images.size(0)\n",
    "\n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def update_history(self, history, train_loss=None, loss=None, f1=None, miou=None,\n",
    "                            precision=None, recall=None, q=None, \n",
    "                            elapsed_time=None, images_per_sec=None, started=None):\n",
    "        if train_loss is not None:\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "        if loss is not None:\n",
    "            history[\"loss\"].append(loss)\n",
    "        if f1 is not None:\n",
    "            history[\"f1\"].append(f1)\n",
    "        if miou is not None:\n",
    "            history[\"miou\"].append(miou)\n",
    "        if precision is not None:\n",
    "            history[\"precision\"].append(precision)\n",
    "        if recall is not None:\n",
    "            history[\"recall\"].append(recall)\n",
    "        if q is not None:\n",
    "            history[\"q\"].append(q)\n",
    "        if elapsed_time is not None:\n",
    "            history[\"elapsed_time\"].append(elapsed_time)\n",
    "        if images_per_sec is not None:\n",
    "            history[\"images_per_sec\"].append(images_per_sec)\n",
    "        if started is not None:\n",
    "            history[\"started\"].append(started)\n",
    "\n",
    "\n",
    "    \n",
    "    def print_last_history_stats(self):\n",
    "            def format_line(title, epoch_idx):\n",
    "                epoch = epoch_idx + 1\n",
    "                values = {k: self.val_history[k][epoch_idx] for k in self.val_history}\n",
    "                lr = self.val_history.get(\"lr\", [None] * len(self.val_history[\"loss\"]))[epoch_idx]\n",
    "                gpu_fps = values.get(\"GPU_FPS\", None)\n",
    "                cpu_fps = values.get(\"CPU_FPS\", None)\n",
    "                line = (\n",
    "                    f\"{title}:\\n\"\n",
    "                    f\" Epoch [{epoch}]\"\n",
    "                    f\" - Loss: {values.get('train_loss', float('nan')):.4f}\"\n",
    "                    f\" Val Loss: {values.get('loss', float('nan')):.4f}\"\n",
    "                    f\" F1-score: {values.get('f1', float('nan')):.4f}\"\n",
    "                    f\" mIoU: {values.get('miou', float('nan')):.4f}\"\n",
    "                    f\" Precision: {values.get('precision', float('nan')):.4f}\"\n",
    "                    f\" Recall: {values.get('recall', float('nan')):.4f}\"\n",
    "                    f\" Q: {values.get('q', float('nan')):.4f}\"\n",
    "                    f\" Tempo total: {values.get('elapsed_time', 'nan')}\"\n",
    "                )\n",
    "                if lr is not None:\n",
    "                    line += f\" LR:{lr:.6f}\"\n",
    "                if gpu_fps is not None:\n",
    "                    line += f\" GPU_FPS: {gpu_fps:.2f}\"\n",
    "                if cpu_fps is not None:\n",
    "                    line += f\" CPU_FPS: {cpu_fps:.2f}\"\n",
    "                return line\n",
    "\n",
    "            # best time (highest F1)\n",
    "            best_epoch = int(max(range(len(self.val_history[\"f1\"])), key=lambda i: self.val_history[\"f1\"][i]))\n",
    "            print(format_line(\"Best model\", best_epoch))\n",
    "\n",
    "            # last season\n",
    "            last_epoch = len(self.val_history[\"f1\"]) - 1\n",
    "            print(format_line(\"Latest model\", last_epoch))\n",
    "\n",
    "\n",
    "    def do_save_xlsx(self):\n",
    "\n",
    "        avg_speed = sum(self.val_history['images_per_sec']) / len(self.val_history['images_per_sec'])\n",
    "        self.info['training_speed_img_per_sec'] = round(avg_speed, 2)\n",
    "\n",
    "        df_val_history = pd.DataFrame(self.val_history)\n",
    "        df_val_history.insert(0, 'epoch', range(1, len(df_val_history)+1))\n",
    "        df_val_history['epoch'] = df_val_history['epoch'].astype(str)\n",
    "\n",
    "\n",
    "        df_test_history = pd.DataFrame(self.test_history)\n",
    "        df_test_history.insert(0, 'epoch', range(1, len(df_test_history)+1))\n",
    "        df_test_history['epoch'] = df_test_history['epoch'].astype(str)\n",
    "\n",
    "\n",
    "        df_info = pd.DataFrame(self.info, index=[0])\n",
    "        with pd.ExcelWriter(self.hist_name, engine='xlsxwriter') as writer:\n",
    "            df_val_history.to_excel(writer, sheet_name='val_history', index=False, float_format=\"%.4f\")\n",
    "            df_test_history.to_excel(writer, sheet_name='test_history', index=False, float_format=\"%.4f\")\n",
    "            df_info.to_excel(writer, sheet_name='model_info', index=False, float_format=\"%.4f\")\n",
    "\n",
    "            workbook  = writer.book\n",
    "            worksheet = writer.sheets['val_history']\n",
    "\n",
    "            chart = workbook.add_chart({'type': 'line'})\n",
    "\n",
    "            # The 'epoch' column is now in column 0\n",
    "            # Assuming 'val_f1' is in column 5 and 'val_IoU' in 6 (or adjust this dynamically)\n",
    "            col_f1 = df_val_history.columns.get_loc('f1')\n",
    "            col_iou  = df_val_history.columns.get_loc('miou')\n",
    "\n",
    "            chart.add_series({\n",
    "                'name':       'f1',\n",
    "                'categories': ['val_history', 1, 0, len(df_val_history), 0],  # column 0 = epoch\n",
    "                'values':     ['val_history', 1, col_f1, len(df_val_history), col_f1],\n",
    "            })\n",
    "            chart.add_series({\n",
    "                'name':       'mIoU',\n",
    "                'categories': ['val_history', 1, 0, len(df_val_history), 0],\n",
    "                'values':     ['val_history', 1, col_iou, len(df_val_history), col_iou],\n",
    "            })\n",
    "\n",
    "            chart.set_title({'name': 'Training'})\n",
    "\n",
    "            chart.set_x_axis({\n",
    "                'name': 'Epoch',\n",
    "                'interval_unit': 10,\n",
    "                'num_font': {'rotation': -45},\n",
    "            })\n",
    "            chart.set_y_axis({'name': 'Value'})\n",
    "\n",
    "            worksheet.insert_chart('K2', chart)\n",
    "\n",
    "    \n",
    "    \n",
    "    def load_xlsx_history(self):\n",
    "        # Read all sheets in the file\n",
    "        xls = pd.read_excel(self.hist_name, sheet_name=None)\n",
    "\n",
    "        # Retrieves the history DataFrame and converts it to a dictionary list\n",
    "        df_val_history   = xls['val_history']\n",
    "        last_epoch   = int(df_val_history['epoch'].iloc[-1])\n",
    "        self.val_history = df_val_history.drop(columns=['epoch']).to_dict(orient='list')\n",
    "\n",
    "\n",
    "        df_test_history   = xls['test_history']\n",
    "        self.test_history = df_test_history.drop(columns=['epoch']).to_dict(orient='list')\n",
    "\n",
    "        # Accumulated time\n",
    "        elapsed_str      = df_val_history['elapsed_time'].iloc[-1]\n",
    "        h, m, s          = map(int, elapsed_str.split(':'))\n",
    "        accumulated_time = timedelta(hours=h, minutes=m, seconds=s).total_seconds()\n",
    "        start_time       = time.time() - accumulated_time  # Adjusts to maintain accumulated count\n",
    "\n",
    "        # Retrieves model information DataFrame and converts to dictionary\n",
    "        df_info = xls['model_info']\n",
    "        self.info = df_info.iloc[0].to_dict()\n",
    "        return last_epoch, start_time\n",
    "\n",
    "    def load_model(self, model_file_dir, model=None, load_xlsx=True, load_scheduler=False):\n",
    "        #if the model is passed\n",
    "        if model is not None:\n",
    "            #self.model receives the new model\n",
    "            self.model = model\n",
    "        #if the model to be loaded has not been passed\n",
    "        if self.model is None:\n",
    "            raise Exception(\"You need to pass the model object in the 'model' parameter\")\n",
    "        \n",
    "        if self.optimizer is None:\n",
    "            self.create_optimizer()\n",
    "        if self.scheduler is None:\n",
    "            self.create_scheduler()\n",
    "        \n",
    "        #loads the model from the .pth file\n",
    "        checkpoint = torch.load(model_file_dir, weights_only=False)\n",
    "        #retrieves the states of the file\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if load_scheduler:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_score  = checkpoint['best_acc']\n",
    "        epoch       = checkpoint['epoch'] + 1\n",
    "        self.model.to(self.get_device())\n",
    "        print(f\"Loaded model: {model_file_dir}\")\n",
    "        if load_xlsx:\n",
    "            start_epoch, start_time = self.load_xlsx_history()\n",
    "            return best_score, epoch, start_epoch, start_time\n",
    "        return best_score, epoch\n",
    "    \n",
    "    def save_model(self, path, epoch, best_score):\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'best_acc': best_score\n",
    "                }, path)\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self.device\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self, model, \n",
    "                    train_loader, \n",
    "                    val_loader,\n",
    "                    test_loader, \n",
    "                    num_epochs=50, \n",
    "                    #saves the model every\n",
    "                    save_every=None,\n",
    "                    #prints the tempo every\n",
    "                    print_every=None,\n",
    "                    #continue training where you left off\n",
    "                    continue_from_last=False,\n",
    "                    #verbose==1 prints the training on the same line\n",
    "                    verbose=3,\n",
    "                    learning_rate=1e-4,\n",
    "                    # scheduler patience=decreases IR after 10 epochs without improvement in acc\n",
    "                    scheduler_patience=10,\n",
    "                    # early_stop_patience=ends training after 20 epochs if acc improves\n",
    "                    early_stop_patience=20,\n",
    "                    measure_cpu_speed=True,\n",
    "                    print_val_stats=False\n",
    "                    ):\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        device = self.get_device()\n",
    "\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.model          = model\n",
    "        start_epoch         = 0\n",
    "        best_score          = -1.0\n",
    "        best_stats          = \"\"\n",
    "        start_time          = time.time()\n",
    "        started             = False\n",
    "        batch_size          = train_loader.batch_size\n",
    "\n",
    "        trainable_parameters = count_trainable_parameters(model)\n",
    "        print(\"Trainable_parameters:\", trainable_parameters)\n",
    "        print(\"Loss function:\", self.loss_function)\n",
    "        self.info['dataset_name']         = train_loader.dataset.__module__\n",
    "        self.info['dataset_batch_size']   = batch_size\n",
    "        self.info['trainable_parameters'] = trainable_parameters\n",
    "        images, labels = next(iter(train_loader))\n",
    "        self.info['dataset_resolution']   = f\"{images.shape[2]} x {images.shape[3]}\"\n",
    "        \n",
    "\n",
    "        self.val_history = {\n",
    "            \"train_loss\":     [],\n",
    "            \"loss\":           [],\n",
    "            \"f1\":             [],\n",
    "            \"miou\":           [],\n",
    "            \"precision\":      [],\n",
    "            \"recall\":         [],\n",
    "            \"q\":              [],\n",
    "            \"elapsed_time\":   [],\n",
    "            \"images_per_sec\": [],\n",
    "            \"started\":        [],\n",
    "        }\n",
    "        self.test_history = {k: [] for k in self.val_history}\n",
    "        \n",
    "\n",
    "        #prints everything on the same line\n",
    "        tqdm_disable = print_every!=None\n",
    "        print_end    = '\\r\\n'\n",
    "        if verbose == 1:\n",
    "            print_end    = '\\r'\n",
    "            tqdm_disable = True\n",
    "\n",
    "\n",
    "        \n",
    "        #if the model name was passed\n",
    "        if self.model_filename is not None:\n",
    "            #create directories\n",
    "            os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "            #First, it checks whether the final, trained model already exists\n",
    "            if os.path.exists(self.model_file_dir):\n",
    "                if self.load_best:\n",
    "                    #if it already exists, load and return\n",
    "                    print(\"Trained model already exists (Loading better version).\")\n",
    "                    self.load_model(self.best_path)\n",
    "                else:\n",
    "                    #if it already exists, load and return\n",
    "                    print(\"Trained model already exists (Loading latest version).\")\n",
    "                    self.load_model(self.model_file_dir)\n",
    "                self.print_last_history_stats()\n",
    "                return model\n",
    "            #if it does not exist and is a continuation of the training\n",
    "            elif continue_from_last == True:\n",
    "                #continues from -last\n",
    "                if os.path.exists(self.last_path):\n",
    "                    _, _, start_epoch, start_time = self.load_model(self.last_path)\n",
    "                    print(f\"Continuing from the saved model: {self.last_path}\")\n",
    "                    print(f\"start_epoch: {start_epoch}, start_time: {start_time}\")\n",
    "                    if start_epoch >= num_epochs:\n",
    "                        self.print_last_history_stats()\n",
    "                        return self.model\n",
    "            \n",
    "\n",
    "\n",
    "        model.to(device)\n",
    "        self.create_optimizer()\n",
    "        self.create_scheduler(patience=scheduler_patience)\n",
    "        early_stopper = EarlyStopping(patience=early_stop_patience, mode='max')\n",
    "    \n",
    "        \n",
    "\n",
    "        ## Training\n",
    "        epoch = start_epoch\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            \n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", disable=tqdm_disable):\n",
    "                images = images.to(device)\n",
    "                masks  = masks.to(device)\n",
    "                ## training loop\n",
    "                train_loss += self.train_loop(images, masks, epoch)\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "            \n",
    "\n",
    "            ## Validation\n",
    "            avg_val_loss, avg_val_f1, avg_val_mIoU, avg_val_precision, avg_val_recall, avg_val_q = evaluate_model(self.model, val_loader, num_classes=self.num_classes, criterion=self.criterion)\n",
    "\n",
    "            ## Test \n",
    "            avg_test_loss, avg_test_f1, avg_test_mIoU, avg_test_precision, avg_test_recall, avg_test_q = evaluate_model(self.model, test_loader, num_classes=self.num_classes, criterion=self.criterion)\n",
    "\n",
    "            elapsed     = time.time() - start_time\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "            current_lr  = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            \n",
    "            \n",
    "            test_stats = (f\"Epoch [{epoch+1}/{num_epochs}] [test_set]: \" \n",
    "                    f\"Loss: {avg_train_loss:.4f}  \" \n",
    "                    f\"F1: {avg_test_f1:.4f} mIoU: {avg_test_mIoU:.4f} \" \n",
    "                    f\"Prec: {avg_test_precision:.4f} \" \n",
    "                    f\"Recall: {avg_test_recall:.4f} Q: {avg_test_q:.4f} \" \n",
    "                    f\"Time: {elapsed_str} LR:{current_lr:.6f}\")\n",
    "            \n",
    "            # The validation stats is used only during training\n",
    "            # for papers we use the test_stats, so we will print only test_stats\n",
    "            # but you can print val_stats and test_stats together with print_val_stats=True\n",
    "            val_stats = (f\" - [val_set]: Loss: {avg_val_loss:.4f} \" \n",
    "                    f\"F1: {avg_val_f1:.4f} mIoU: {avg_val_mIoU:.4f} \" \n",
    "                    f\"Prec: {avg_val_precision:.4f} \" \n",
    "                    f\"Recall: {avg_val_recall:.4f} Q: {avg_val_q:.4f} \")\n",
    "\n",
    "            if print_val_stats:\n",
    "                test_stats += val_stats\n",
    "\n",
    "            if print_every is None:\n",
    "                print(test_stats,end=print_end)\n",
    "            else:\n",
    "                if (epoch+1) % print_every == 0:\n",
    "                    print(val_stats,end=print_end)\n",
    "                    print(test_stats,end=print_end)\n",
    "            \n",
    "\n",
    "            images_per_sec = (len(train_loader) * batch_size) / elapsed\n",
    "            \n",
    "            ## Saves the evolution of the network\n",
    "            self.update_history(\n",
    "                self.val_history,\n",
    "                train_loss=avg_train_loss,\n",
    "                loss=avg_val_loss,\n",
    "                f1=avg_val_f1,\n",
    "                miou=avg_val_mIoU,\n",
    "                precision=avg_val_precision,\n",
    "                recall=avg_val_recall,\n",
    "                q=avg_val_q,\n",
    "                elapsed_time=elapsed_str,\n",
    "                images_per_sec=images_per_sec,\n",
    "                started=('started' if not started else '')\n",
    "            )\n",
    "            self.update_history(\n",
    "                self.test_history,\n",
    "                train_loss=avg_train_loss,\n",
    "                loss=avg_test_loss,\n",
    "                f1=avg_test_f1,\n",
    "                miou=avg_test_mIoU,\n",
    "                precision=avg_test_precision,\n",
    "                recall=avg_test_recall,\n",
    "                q=avg_test_q,\n",
    "                elapsed_time=elapsed_str,\n",
    "                images_per_sec=images_per_sec,\n",
    "                started=('started' if not started else '')\n",
    "            )\n",
    "            \n",
    "\n",
    "            started = True\n",
    "\n",
    "            # The avg_val_f1 will be observed for the scheduler and early_stopper\n",
    "\n",
    "            # reduces the learning rate if the score does not improve\n",
    "            self.scheduler.step(avg_val_f1)\n",
    "\n",
    "            # for training if you don't improve in X times\n",
    "            early_stopper.step(avg_val_f1)\n",
    "            if early_stopper.early_stop:\n",
    "                print(f\"Stopping at epoch {epoch+1} by early stopping.\")\n",
    "                break\n",
    "\n",
    "            ## Save the best model so far\n",
    "            if avg_val_f1 > best_score:\n",
    "                best_score = avg_val_f1\n",
    "                \n",
    "                if self.model_file_dir is not None:\n",
    "                    #save the model at the best time\n",
    "                    self.save_model(self.best_path, epoch, best_score)\n",
    "                    current_lr  = self.optimizer.param_groups[0]['lr']\n",
    "                                        \n",
    "                    best_test_stats = (f\"Epoch [{epoch+1}/{num_epochs}] [test_set]:\" \n",
    "                                f\"Loss: {avg_train_loss:.4f} Val Loss: {avg_test_loss:.4f} \" \n",
    "                                f\"F1: {avg_test_f1:.4f} mIoU: {avg_test_mIoU:.4f} \" \n",
    "                                f\"Prec: {avg_test_precision:.4f} \" \n",
    "                                f\"Recall: {avg_test_recall:.4f} Q: {avg_test_q:.4f} \" \n",
    "                                f\"Time: {elapsed_str} LR:{current_lr:.6f}\")\n",
    "                    \n",
    "                    best_val_stats = (f\" - [val_set]: Loss: {avg_val_loss:.4f} \" \n",
    "                                f\"F1: {avg_val_f1:.4f} mIoU: {avg_val_mIoU:.4f} \" \n",
    "                                f\"Prec: {avg_val_precision:.4f} \" \n",
    "                                f\"Recall: {avg_val_recall:.4f} Q: {avg_val_q:.4f} \")\n",
    "                    \n",
    "\n",
    "                    if print_val_stats:\n",
    "                        best_test_stats += best_val_stats\n",
    "                    if print_every is None and verbose > 1:\n",
    "                        print(\"✔ Best model saved:\", best_test_stats, end=print_end)\n",
    "                    #save excel to the current moment\n",
    "                    if self.save_xlsx:\n",
    "                        self.do_save_xlsx()\n",
    "            \n",
    "                \n",
    "\n",
    "            #Saves the network every\n",
    "            if save_every is not None and (epoch + 1) % save_every == 0:\n",
    "                last_model_file_dir = self.model_file_dir.replace('.pth','-last.pth')\n",
    "                self.save_model(last_model_file_dir, epoch, best_score)\n",
    "                self.do_save_xlsx()\n",
    "                if verbose > 1:\n",
    "                    print(\"Saved last as\", last_model_file_dir, end=print_end)\n",
    "\n",
    "\n",
    "                \n",
    "        last_test_stats = (f\"Epoch [{epoch+1}/{num_epochs}] [test_set]: \" \n",
    "                    f\"Loss: {avg_train_loss:.4f} Val Loss: {avg_test_loss:.4f} \" \n",
    "                    f\"F1: {avg_test_f1:.4f} mIoU: {avg_test_mIoU:.4f} \" \n",
    "                    f\"Prec: {avg_test_precision:.4f} \" \n",
    "                    f\"Recall: {avg_test_recall:.4f} Q: {avg_test_q:.4f} \" \n",
    "                    f\"Time: {elapsed_str} LR:{current_lr:.6f}\")\n",
    "        \n",
    "        last_val_stats = (f\" - [val_set]: Loss: {avg_val_loss:.4f} \" \n",
    "                    f\"F1: {avg_val_f1:.4f} mIoU: {avg_val_mIoU:.4f} \" \n",
    "                    f\"Prec: {avg_val_precision:.4f} \" \n",
    "                    f\"Recall: {avg_val_recall:.4f} Q: {avg_val_q:.4f} \")\n",
    "        if print_val_stats:\n",
    "            last_test_stats += last_val_stats\n",
    "\n",
    "\n",
    "        #calculates the FPS of the model\n",
    "        self.info['GPU_FPS'], self.info['GPU_time_per_image'], self.info['CPU_FPS'], self.info['CPU_time_per_image'] = measure_inference_speed(self.model, \n",
    "                                                                                                                                               val_loader, \n",
    "                                                                                                                                               measure_cpu_speed=measure_cpu_speed)\n",
    "          \n",
    "        print(\"\")\n",
    "        if best_test_stats:\n",
    "            print(\"Best model:\\r\\n\", best_test_stats)\n",
    "        print(\"Latest model:\\r\\n\", last_test_stats + '\\r\\n GPU_FPS:',self.info['GPU_FPS'], ' CPU_FPS:',self.info['CPU_FPS'])\n",
    "\n",
    "\n",
    "        if self.model_file_dir is not None:\n",
    "            self.save_model(self.model_file_dir, epoch, best_score)\n",
    "            print(\"Saved as\", self.model_file_dir)\n",
    "\n",
    "        \n",
    "        if self.save_xlsx:\n",
    "            # Write the excel file with history\n",
    "            self.do_save_xlsx()\n",
    "\n",
    "        #beep win\n",
    "        #os.system('powershell.exe -Command \"[console]::beep(600,200); [console]::beep(600,200);\"')\n",
    "        #linux\n",
    "        os.system('play -nq -t alsa synth 0.2 sine 600; play -nq -t alsa synth 0.2 sine 600')\n",
    "        return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e18643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def check_masks_for_ce_loss(masks, num_classes=3, ignore_index=255):\n",
    "#     \"\"\"\n",
    "#     Checks a mask for invalid values ​​before CrossEntropyLoss.\n",
    "    \n",
    "#     masks: tensor [B,H,W] ou [B,1,H,W]\n",
    "#     \"\"\"\n",
    "#     if masks.ndim == 4:\n",
    "#         masks = masks.squeeze(1)  # [b,h,w]\n",
    "\n",
    "#     invalid_mask = (masks < 0) | ((masks >= num_classes) & (masks != ignore_index))\n",
    "#     has_invalid = invalid_mask.any()\n",
    "\n",
    "#     if has_invalid:\n",
    "#         print(\"Invalid values ​​found in masks!\")\n",
    "#         for b in range(masks.size(0)):\n",
    "#             unique_vals = torch.unique(masks[b])\n",
    "#             if ((unique_vals >= num_classes) & (unique_vals != ignore_index)).any() or (unique_vals < 0).any():\n",
    "#                 print(f\"Batch {b}: unique values -> {unique_vals.tolist()}\")\n",
    "#     else:\n",
    "#         print(\"All masks valid for CrossEntropyLoss.\")\n",
    "\n",
    "\n",
    "# class MulticlassTrainer(Trainer):\n",
    "\n",
    "#     def __init__(self, num_classes, model_filename=None, model_dir=None, info={}, save_xlsx=False, loss_function='CrossEntropyLoss'):\n",
    "#         #Correct the loss_function if necessary\n",
    "#         if loss_function == 'BCEWithLogitsLoss':\n",
    "#             loss_function = 'CrossEntropyLoss'\n",
    "\n",
    "#         super(MulticlassTrainer, self).__init__(model_filename=model_filename, model_dir=model_dir, info=info, save_xlsx=save_xlsx, loss_function=loss_function)\n",
    "#         self.num_classes = num_classes\n",
    "        \n",
    "        \n",
    "#     def create_criterion(self):\n",
    "#         if self.loss_function == 'CrossEntropyLoss':\n",
    "#             self.info['loss_function'] = 'CrossEntropyLoss'\n",
    "#             self.criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "#         else:\n",
    "#             raise ValueError(f'Loss function {self.loss_function} not found.') \n",
    "\n",
    "        \n",
    "    \n",
    "#     # def train_loop(self, images, masks, epoch):\n",
    "#     #     outputs     = self.model(images)\n",
    "#     #     masks_s     = masks.long().squeeze(1)\n",
    "\n",
    "#     #     loss    = self.criterion(outputs, masks_s)\n",
    "\n",
    "#     #     self.optimizer.zero_grad()\n",
    "#     #     loss.backward()\n",
    "#     #     self.optimizer.step()\n",
    "#     #     train_loss = loss.item() * images.size(0)\n",
    "\n",
    "#     #     return train_loss\n",
    "\n",
    "\n",
    "#     # def val_loop(self, images, masks):\n",
    "#     #     outputs     = model(images)\n",
    "\n",
    "#     #     masks_s     = masks.long().squeeze(1)\n",
    "\n",
    "        \n",
    "#     #     try:\n",
    "#     #         loss        = self.criterion(outputs, masks_s)\n",
    "#     #         val_loss    = loss.item() * images.size(0)\n",
    "#     #     except Exception as e:\n",
    "#     #         check_masks_for_ce_loss(masks_s, num_classes=self.num_classes, ignore_index=255)\n",
    "#     #         raise\n",
    "        \n",
    "#     #     preds       = torch.argmax(outputs, dim=1)\n",
    "#     #     dice, mIoU, precision, recall, f1, q = compute_segmentation_metrics(preds, masks, self.num_classes)\n",
    "#     #     IoU = compute_iou(preds, masks, num_classes=self.num_classes)\n",
    "\n",
    "#     #     val_dice      = dice      * images.size(0)\n",
    "#     #     val_mIoU      = mIoU      * images.size(0)\n",
    "#     #     val_IoU       = IoU       * images.size(0)\n",
    "#     #     val_precision = precision * images.size(0)\n",
    "#     #     val_recall    = recall    * images.size(0)\n",
    "#     #     val_f1        = f1        * images.size(0)\n",
    "#     #     val_q         = q         * images.size(0)\n",
    "\n",
    "#     #     return val_loss, val_dice, val_mIoU, val_IoU, val_precision, val_recall, val_f1, val_q\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch5070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
