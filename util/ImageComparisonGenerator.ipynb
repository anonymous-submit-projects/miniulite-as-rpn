{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45022368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#jupyter nbconvert --to script ImageComparisonGenerator.ipynb\n",
    "\n",
    "class ImageComparisonGenerator:\n",
    "\n",
    "    def __init__(self, model, model_name1=\"Prediction\", model2=None, model_name2=\"Prediction\"):\n",
    "        self.model  = model\n",
    "        self.model_name1 = model_name1\n",
    "        self.model2 = model2\n",
    "        self.model_name2 = model_name2\n",
    "\n",
    "    def get_model_output(self,images, model=None):\n",
    "        if model is None:\n",
    "            return self.model(images)\n",
    "        else:\n",
    "            return model(images)\n",
    "    \n",
    "    # Helper function within the class\n",
    "    def _get_sample_by_index(self, dataloader, idx):\n",
    "        count = 0\n",
    "        for imgs, masks in dataloader:\n",
    "            batch_size = imgs.shape[0]\n",
    "            if idx < count + batch_size:\n",
    "                local_idx = idx - count\n",
    "                return imgs[local_idx:local_idx+1], masks[local_idx]\n",
    "            count += batch_size\n",
    "        raise IndexError(f\"Index {idx} out of range for dataset of length {len(dataloader.dataset)}\")\n",
    "\n",
    "\n",
    "    # Auxiliary functions\n",
    "    def _prepare_mask_vis(self, mask, num_classes=1, ignore_val=255):\n",
    "        ignore_mask = (mask == ignore_val)\n",
    "        mask_vis = mask.copy()\n",
    "        \n",
    "        if num_classes > 1:\n",
    "            color_map = {0:[0.9,0.9,0],1:[0.3,0.3,1],2:[0.9,0,0.9]}\n",
    "            mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.float32)\n",
    "            for class_val, color in color_map.items():\n",
    "                mask_rgb[mask == class_val] = color\n",
    "            mask_rgb[ignore_mask] = [0.8,0.8,0.8]\n",
    "            mask_vis = mask_rgb\n",
    "        else:\n",
    "            mask_vis[ignore_mask] = -1\n",
    "        return mask_vis, ignore_mask\n",
    "\n",
    "    def _prepare_prediction_vis(self, output, mask, num_classes=1, do_diff=True, invert_diff_colors=False, ignore_mask=None):\n",
    "        if num_classes == 1:\n",
    "            out_sigmoid = torch.sigmoid(output[0])\n",
    "            pred = (out_sigmoid > 0.5).float().cpu().squeeze().numpy()\n",
    "        else:\n",
    "            out_softmax = torch.softmax(output[0], dim=0)\n",
    "            pred = torch.argmax(out_softmax, dim=0).cpu().numpy()\n",
    "\n",
    "        if do_diff:\n",
    "            if len(mask.shape) == 2:\n",
    "                h, w = mask.shape\n",
    "            else:\n",
    "                h, w, _ = mask.shape\n",
    "\n",
    "            diff_img = np.zeros((h, w, 3), dtype=np.float32)\n",
    "            if num_classes == 1:\n",
    "                tp = (pred == 1) & (mask == 1)\n",
    "                fn = (pred == 0) & (mask == 1)\n",
    "                fp = (pred == 1) & (mask == 0)\n",
    "                if invert_diff_colors:\n",
    "                    diff_img[tp] = [1,1,1]\n",
    "                    diff_img[fp] = [1,0.5,0]\n",
    "                    diff_img[fn] = [1,0,0]\n",
    "                else:\n",
    "                    diff_img[tp] = [1,1,1]\n",
    "                    diff_img[fn] = [1,0.5,0]\n",
    "                    diff_img[fp] = [1,0,0]\n",
    "            else:\n",
    "                color_map = {0:[0.9,0.9,0],1:[0.3,0.3,1],2:[0.9,0,0.9]}\n",
    "                mismatches = (mask != pred)\n",
    "                mismatches[ignore_mask] = False\n",
    "                for cls in range(num_classes):\n",
    "                    cls_mask = (mask == cls) & (pred == cls)\n",
    "                    diff_img[cls_mask] = color_map.get(cls,[1,1,1])\n",
    "                diff_img[mismatches] = [1,0,0]\n",
    "                diff_img[ignore_mask] = [0.8,0.8,0.8]\n",
    "            return diff_img\n",
    "        else:\n",
    "            if num_classes > 1:\n",
    "                color_map = {0:[0.9,0.9,0],1:[0.3,0.3,1],2:[0.9,0,0.9]}\n",
    "                pred_vis = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.float32)\n",
    "                for class_val, color in color_map.items():\n",
    "                    pred_vis[pred == class_val] = color\n",
    "                return pred_vis\n",
    "            else:\n",
    "                return pred\n",
    "\n",
    "    def _prepare_image_disp(self, img):\n",
    "        img_disp = img.cpu().squeeze()\n",
    "        if img_disp.ndim == 3 and img_disp.shape[0] == 3:\n",
    "            img_disp = img_disp.permute(1,2,0)\n",
    "        elif img_disp.ndim == 3 and img_disp.shape[0] == 1:\n",
    "            img_disp = img_disp.squeeze(0)\n",
    "        img_disp = img_disp*0.5 + 0.5\n",
    "        return img_disp\n",
    "    \n",
    "\n",
    "\n",
    "    # Output Functions\n",
    "    def save_output_row(self, sample_loader, samples=[0],\n",
    "                        num_classes=1, do_diff=True, invert_diff_colors=False,\n",
    "                        do_save=False):\n",
    "        if self.model is None:\n",
    "            raise Exception(\"The model is not loaded.\")\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "        num_rows = len(samples)\n",
    "        fig = plt.figure(figsize=(11, 4*num_rows))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample_idx in enumerate(samples):\n",
    "                img, mask = self._get_sample_by_index(sample_loader, sample_idx)\n",
    "                img = img.to(device)\n",
    "                mask = mask.cpu().squeeze().numpy()\n",
    "\n",
    "                mask_vis, ignore_mask = self._prepare_mask_vis(mask, num_classes)\n",
    "                pred_vis = self._prepare_prediction_vis(self.get_model_output(img), mask, num_classes, do_diff, invert_diff_colors, ignore_mask)\n",
    "                img_disp = self._prepare_image_disp(img)\n",
    "\n",
    "                for col, im in enumerate([img_disp, mask_vis, pred_vis]):\n",
    "                    ax = fig.add_axes([\n",
    "                        col/3, 1-(idx+1)/num_rows, 1/3, 1/num_rows\n",
    "                    ])\n",
    "                    ax.imshow(im, cmap='gray' if num_classes==1 else None, aspect='auto')\n",
    "                    ax.set_xticks([]); ax.set_yticks([])\n",
    "                    for spine in ax.spines.values():\n",
    "                        spine.set_visible(True)\n",
    "                        spine.set_edgecolor('black')\n",
    "                        spine.set_linewidth(1.5)\n",
    "                    if idx==0:\n",
    "                        font_size = 30\n",
    "                        if col==0: ax.set_title(\"Image\", fontsize=font_size, y=1.0)\n",
    "                        elif col==1: ax.set_title(\"Ground Truth\", fontsize=font_size, y=1.0)\n",
    "                        else: ax.set_title(\"Prediction\", fontsize=font_size, y=1.0)\n",
    "\n",
    "        plt.tight_layout(rect=[0,0,1,0.96])\n",
    "        if do_save: fig.savefig(do_save, format='eps', bbox_inches='tight', pad_inches=0.1)\n",
    "        else: plt.show()\n",
    "\n",
    "\n",
    "    def save_output_quad(self, sample_loader, samples=[0],\n",
    "                        num_classes=1, do_diff=True, invert_diff_colors=False,\n",
    "                        do_save=False):\n",
    "        if self.model is None:\n",
    "            raise Exception(\"The model is not loaded.\")\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        self.model.eval()\n",
    "        if self.model2 is not None:\n",
    "            self.model2.eval()\n",
    "\n",
    "        fig = plt.figure(figsize=(10,10))  # greater width for 2 models\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, sample_idx in enumerate(samples):\n",
    "                img, mask = self._get_sample_by_index(sample_loader, sample_idx)\n",
    "                img = img.to(device)\n",
    "                mask_np = mask.cpu().squeeze().numpy()\n",
    "\n",
    "                mask_vis, ignore_mask = self._prepare_mask_vis(mask_np, num_classes)\n",
    "\n",
    "                # Model 1 Prediction\n",
    "                pred1_vis = self._prepare_prediction_vis(\n",
    "                    self.get_model_output(img, model=self.model), \n",
    "                    mask_np, num_classes, do_diff, invert_diff_colors, ignore_mask\n",
    "                )\n",
    "                img_disp = self._prepare_image_disp(img)\n",
    "\n",
    "                # Model 2 prediction, if it exists\n",
    "                if self.model2 is not None:\n",
    "                    img2 = img.to(device)\n",
    "                    pred2_vis = self._prepare_prediction_vis(\n",
    "                        self.get_model_output(img2, model=self.model2),\n",
    "                        mask_np, num_classes, do_diff, invert_diff_colors, ignore_mask\n",
    "                    )\n",
    "                else:\n",
    "                    pred2_vis = None\n",
    "\n",
    "                vertical_gap = 0.13\n",
    "                height = (1 - vertical_gap) / 2\n",
    "\n",
    "                # Position structure: (row, column, image, title)\n",
    "                positions = [\n",
    "                    (0,0,img_disp,\"Image\"),\n",
    "                    (0,1,mask_vis,\"Ground Truth\"),\n",
    "                    (1,0,pred1_vis,self.model_name1),\n",
    "                    (1,1,pred2_vis,self.model_name2 if self.model2 is not None else \"Prediction\")\n",
    "                ]\n",
    "\n",
    "                for row, col, im, title in positions:\n",
    "                    bottom = 1 - (row + 1)*(height + vertical_gap/2)\n",
    "                    ax = fig.add_axes([col/2, bottom, 1/2, height])\n",
    "                    if im is not None:\n",
    "                        ax.imshow(im, cmap='gray' if num_classes==1 else None, aspect='auto')\n",
    "                    ax.set_xticks([]); ax.set_yticks([])\n",
    "                    for spine in ax.spines.values():\n",
    "                        spine.set_visible(True)\n",
    "                        spine.set_edgecolor('black')\n",
    "                        spine.set_linewidth(1.5)\n",
    "                    font_size = 30\n",
    "                    ax.set_title(title, fontsize=font_size, y=1.0)\n",
    "\n",
    "        plt.tight_layout(rect=[0,0,1,0.96])\n",
    "        if do_save:\n",
    "            fig.savefig(do_save, format='eps', bbox_inches='tight', pad_inches=0.1)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_file_name):\n",
    "    checkpoint = torch.load(model_file_name, map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Filters only existing keys in the model\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_keys}\n",
    "\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch5070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
