{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions for extracting patches\n",
    "# ------------------------------------------------------------\n",
    "def extract_patch(image, mask, y_center, x_center, patch_size):\n",
    "    \"\"\"Extracts a patch centered on coordinates (y_center, x_center) with padding if necessary.\"\"\"\n",
    "    _, H, W = mask.shape\n",
    "    half = patch_size // 2\n",
    "\n",
    "    y1 = max(0, y_center - half)\n",
    "    x1 = max(0, x_center - half)\n",
    "    y2 = min(H, y1 + patch_size)\n",
    "    x2 = min(W, x1 + patch_size)\n",
    "\n",
    "    img_patch = image[:, y1:y2, x1:x2]\n",
    "    mask_patch = mask[:, y1:y2, x1:x2]\n",
    "\n",
    "    # Padding if you hit the edge\n",
    "    pad_y = patch_size - img_patch.shape[1]\n",
    "    pad_x = patch_size - img_patch.shape[2]\n",
    "    if pad_y > 0 or pad_x > 0:\n",
    "        img_patch = F.pad(img_patch, (0, pad_x, 0, pad_y))\n",
    "        mask_patch = F.pad(mask_patch, (0, pad_x, 0, pad_y))\n",
    "\n",
    "    return img_patch, mask_patch\n",
    "\n",
    "\n",
    "def extract_object_patches(image, mask, patch_size=64):\n",
    "    \"\"\"Extracts patches centered on the main object and extra if the object is large.\"\"\"\n",
    "    H, W = mask.shape[-2:]\n",
    "    patches = []\n",
    "\n",
    "    # finds object pixels\n",
    "    y_indices, x_indices = torch.where(mask[0] > 0)\n",
    "    if len(y_indices) == 0:\n",
    "        return []  # no object\n",
    "\n",
    "    y_min, y_max = y_indices.min().item(), y_indices.max().item()\n",
    "    x_min, x_max = x_indices.min().item(), x_indices.max().item()\n",
    "    y_center = (y_min + y_max) // 2\n",
    "    x_center = (x_min + x_max) // 2\n",
    "\n",
    "    # centered patch\n",
    "    patches.append(extract_patch(image, mask, y_center, x_center, patch_size))\n",
    "\n",
    "    # adds extra patches if the object is larger than the patch\n",
    "    if y_min < y_center - patch_size // 2:\n",
    "        patches.append(extract_patch(image, mask, y_min + patch_size // 2, x_center, patch_size))\n",
    "    if y_max > y_center + patch_size // 2:\n",
    "        patches.append(extract_patch(image, mask, y_max - patch_size // 2, x_center, patch_size))\n",
    "    if x_min < x_center - patch_size // 2:\n",
    "        patches.append(extract_patch(image, mask, y_center, x_min + patch_size // 2, patch_size))\n",
    "    if x_max > x_center + patch_size // 2:\n",
    "        patches.append(extract_patch(image, mask, y_center, x_max - patch_size // 2, patch_size))\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Creating and saving patches to disk\n",
    "# ------------------------------------------------------------\n",
    "def create_patches_dataset(dataset_dir,output_dir,patch_size=128,resolution=None):\n",
    "    \"\"\"Generates a new dataset with centralized patches and saves it to disk.\"\"\"\n",
    "    image_transforms = []\n",
    "    mask_transforms = []\n",
    "\n",
    "    # Only apply Resize if resolution was informed\n",
    "    if resolution is not None:\n",
    "        image_transforms.append(transforms.Resize((resolution, resolution)))\n",
    "        mask_transforms.append(transforms.Resize((resolution, resolution), interpolation=Image.NEAREST))\n",
    "\n",
    "    # Basic Transforms (always applied)\n",
    "    image_transforms.extend([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transforms.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (x > 0.5).float())\n",
    "    ])\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "    mask_transform = transforms.Compose(mask_transforms)\n",
    "\n",
    "    splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    for split in splits:\n",
    "        img_dir = os.path.join(dataset_dir, \"images\", split)\n",
    "        mask_dir = os.path.join(dataset_dir, \"labels\", split)\n",
    "\n",
    "        out_img_dir = os.path.join(output_dir, \"images\", split)\n",
    "        out_mask_dir = os.path.join(output_dir, \"labels\", split)\n",
    "        os.makedirs(out_img_dir, exist_ok=True)\n",
    "        os.makedirs(out_mask_dir, exist_ok=True)\n",
    "\n",
    "        image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".png\")])\n",
    "\n",
    "        print(f\"\\nProcessing {split} ({len(image_files)} images)...\")\n",
    "\n",
    "        for fname in tqdm(image_files):\n",
    "            img_path = os.path.join(img_dir, fname)\n",
    "            mask_path = os.path.join(mask_dir, fname)\n",
    "\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"[Warning] Mask not found for {fname}\")\n",
    "                continue\n",
    "\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "            img_t = image_transform(img)\n",
    "            mask_t = mask_transform(mask)\n",
    "\n",
    "            patches = extract_object_patches(img_t, mask_t, patch_size)\n",
    "            if len(patches) == 0:\n",
    "                continue  # ignore image without object\n",
    "\n",
    "            base_name = os.path.splitext(fname)[0]\n",
    "            for i, (p_img, p_mask) in enumerate(patches):\n",
    "                img_patch_np = (p_img.permute(1, 2, 0).numpy() * 255).astype(\"uint8\")\n",
    "                mask_patch_np = (p_mask.squeeze().numpy() * 255).astype(\"uint8\")\n",
    "\n",
    "                out_img_path = os.path.join(out_img_dir, f\"{base_name}_patch{i}.png\")\n",
    "                out_mask_path = os.path.join(out_mask_dir, f\"{base_name}_patch{i}.png\")\n",
    "\n",
    "                Image.fromarray(img_patch_np).save(out_img_path)\n",
    "                Image.fromarray(mask_patch_np).save(out_mask_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a84f71",
   "metadata": {},
   "source": [
    "## Creating patch datasets based on the already augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e939c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset_512 = \"/mnt/TUDAO/0Datasets/fuseg/augmented-v3-512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e10d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    patch_size = 48\n",
    "    create_patches_dataset(\n",
    "        dataset_dir=augmented_dataset_512,\n",
    "        output_dir=f\"{augmented_dataset_512}_patches{patch_size}/\",\n",
    "        patch_size=patch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3de6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    patch_size = 128\n",
    "    create_patches_dataset(\n",
    "        dataset_dir=augmented_dataset_512,\n",
    "        output_dir=f\"{augmented_dataset_512}_patches{patch_size}/\",\n",
    "        patch_size=patch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    patch_size = 256\n",
    "    create_patches_dataset(\n",
    "        dataset_dir=augmented_dataset_512,\n",
    "        output_dir=f\"{augmented_dataset_512}_patches{patch_size}/\",\n",
    "        patch_size=patch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    patch_size = 384\n",
    "    create_patches_dataset(\n",
    "        dataset_dir=augmented_dataset_512,\n",
    "        output_dir=f\"{augmented_dataset_512}_patches{patch_size}/\",\n",
    "        patch_size=patch_size,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch5070",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
